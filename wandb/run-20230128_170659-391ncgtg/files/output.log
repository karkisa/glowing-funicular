GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1789: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.
  rank_zero_warn(
Missing logger folder: /Users/sagar/Desktop/Ace/Brovo5/lightning_logs
  | Name  | Type             | Params
-------------------------------------------
0 | model | EfficientNet     | 4.0 M
1 | loss  | CrossEntropyLoss | 0
-------------------------------------------
4.0 M     Trainable params
0         Non-trainable params
4.0 M     Total params
16.061    Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/Users/sagar/Desktop/Ace/Brovo5/playground.py", line 43, in <module>
    main()
  File "/Users/sagar/Desktop/Ace/Brovo5/playground.py", line 39, in main
    Trainer.fit(Classifier)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 696, in fit
    self._call_and_handle_interrupt(
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 735, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1274, in _run_train
    self._run_sanity_check()
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1343, in _run_sanity_check
    val_loop.run()
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 155, in advance
    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 127, in advance
    batch = next(data_fetcher)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py", line 184, in __next__
    return self.fetching_function()
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py", line 263, in fetching_function
    self._fetch_next_batch(self.dataloader_iter)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py", line 277, in _fetch_next_batch
    batch = next(iterator)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 681, in __next__
    data = self._next_data()
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1376, in _next_data
    return self._process_data(data)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _process_data
    data.reraise()
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/torch/_utils.py", line 461, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 302, in _worker_loop
    data = fetcher.fetch(index)
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/Users/sagar/opt/miniconda3/envs/Bravo13/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/Users/sagar/Desktop/Ace/Brovo5/pipeline.py", line 35, in __getitem__
    img = self.read_img(self.paths[index])
TypeError: read_img() takes 1 positional argument but 2 were given